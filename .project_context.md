# Project Context: Google Photos Backup

## Core Philosophy
- **Local First**: All metadata and organization happens locally.
- **Idempotency**: All operations (download, extract, dedup) must be resumable and safe to re-run.
- **In-Place**: We do not move files to a central storage. We keep them in their "Export" folders (`downloads/<ID>/raw`) and use **Relative Symlinks** to deduplicate or organize them.
- **Single Source of Truth**:
  - `history.json`: Lifecycle of Exports (Requested -> Ready -> Downloaded).
  - `processing_index.json`: Index of all extracted files (Hash, Path) for deduplication.

## Architecture

### 1. Sync (Downloader)
- Automates Google Takeout using Go-Rod (Headless Chrome).
- **Strategy**: Iframe injection for concurrent downloads without auth issues.
- **State**: `downloads/<ID>/state.json` tracks byte-level progress of ZIPs.

### 2. Process (Organizer)
- **Phase 1: Extraction & Indexing**
  - Extracts compressed files to `downloads/<ID>/raw`.
  - **Optimization**: Calculates SHA256 stream-hash during extraction (or scans existing files if resuming).
  - **Metadata**: Parses Google's sidecar JSONs to fix `mtime` of media files.
- **Phase 2: Global Deduplication**
  - Loads all `processing_index.json` files.
  - Groups by SHA256.
  - **Heuristic**: Selects "Primary" file based on path quality (e.g. prefers "Albums/Trip 2023/img.jpg" over "Photos from 2023/img.jpg").
  - **Action**: Replaces duplicates with `symlink -> Primary`.

## Key Decisions
- **Force Flags**:
  - `--force-metadata`: Fast scan (no hash), fixes dates.
  - `--force-dedup`: Full scan (hash), fixes duplicates.
  - `--force-extract`: Full reset (re-extracts).
- **FileSystem**:
  - We use `filepath.Rel` for symlinks to ensure the backup folder is portable (can be moved to another drive).
